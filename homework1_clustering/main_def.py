import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering

def plot_hierarchical_clustering(data: np.ndarray, 
                                 cluster_nums=[2,3,4,5], 
                                 linkage_method='ward'):
    """
    å±¤æ¬¡å¼åˆ†ç¾¤è¦–è¦ºåŒ–å·¥å…·
    -----------------------
    åƒæ•¸ï¼š
        data : np.ndarray
            äºŒç¶­è³‡æ–™ï¼Œä¾‹å¦‚ shape = (n_samples, 2)
        cluster_nums : list[int]
            è¦æ¯”è¼ƒçš„åˆ†ç¾¤æ•¸ï¼ˆé è¨­ç‚º [2,3,4,5]ï¼‰
        linkage_method : str
            linkage æ–¹æ³• ('ward', 'complete', 'average', 'single')

    è¼¸å‡ºï¼š
        1. Dendrogramï¼ˆæ¨¹ç‹€åœ–ï¼‰
        2. åŸå§‹è³‡æ–™æ•£é»åœ–
        3. å„ç¾¤æ•¸ k çš„åˆ†ç¾¤çµæœå­åœ–
    """
    
    # ğŸ§© æª¢æŸ¥è³‡æ–™ç¶­åº¦
    if data.ndim != 2 or data.shape[1] != 2:
        raise ValueError("âš ï¸ data å¿…é ˆæ˜¯ shape=(n_samples, 2) çš„ numpy array")

    # ğŸˆ¶ è¨­å®šä¸­æ–‡å­—é«”ï¼ˆmacOSï¼‰
    plt.rcParams['font.family'] = 'Heiti TC'
    plt.rcParams['axes.unicode_minus'] = False

    # -------------------------------
    # ğŸ”¹ Step 1: Dendrogram æ¨¹ç‹€åœ–
    # -------------------------------
    linked = linkage(data, method=linkage_method)
    plt.figure(figsize=(6, 5))
    dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=False)
    plt.title(f'éšå±¤å¼åˆ†ç¾¤æ¨¹ç‹€åœ– (method={linkage_method})')
    plt.xlabel('æ¨£æœ¬é»')
    plt.ylabel('è·é›¢')
    plt.grid(True)
    plt.show()

    # -------------------------------
    # ğŸ”¹ Step 2: åŸå§‹è³‡æ–™æ•£é»åœ–
    # -------------------------------
    plt.figure(figsize=(6, 5))
    plt.scatter(data[:, 0], data[:, 1], s=50, edgecolors='k')
    plt.title('åŸå§‹è³‡æ–™æ•£é»åœ–')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.grid(True)
    plt.show()

    # -------------------------------
    # ğŸ”¹ Step 3: å„ç¾¤æ•¸å­åœ–
    # -------------------------------
    n = len(cluster_nums)
    fig, axes = plt.subplots(2, 2, figsize=(14, 10), constrained_layout=True)
    axes = axes.ravel()

    for i, num in enumerate(cluster_nums):
        hc = AgglomerativeClustering(n_clusters=num, linkage=linkage_method)
        cluster_label = hc.fit_predict(data)

        sc = axes[i].scatter(
            data[:, 0],
            data[:, 1],
            c=cluster_label,
            cmap='viridis',
            s=60,
            edgecolors='k'
        )
        axes[i].set_title(f'Hierarchical Clustering (k={num})')
        axes[i].set_xlabel('Feature 1')
        axes[i].set_ylabel('Feature 2')
        axes[i].grid(True)

    # å…±ç”¨ colorbar
    fig.colorbar(sc, ax=axes, orientation='vertical', fraction=0.02, pad=0.04, label='Cluster Label')
    plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn_extra.cluster import KMedoids

def compare_kmeans_pam(data: np.ndarray, k_values=[2, 3, 4, 5]):
    """
    æ¯”è¼ƒ K-Means èˆ‡ PAM (K-Medoids) çš„åˆ†ç¾¤çµæœ
    
    åƒæ•¸ï¼š
    ---------
    data : np.ndarray
        shape=(n_samples, 2) çš„è³‡æ–™ã€‚
    k_values : list[int]
        è¦æ¸¬è©¦çš„ç¾¤æ•¸ï¼Œä¾‹å¦‚ [2,3,4,5]
    
    åŠŸèƒ½ï¼š
    ---------
    1. ä¸Šæ’é¡¯ç¤º K-Means åˆ†ç¾¤çµæœ
    2. ä¸‹æ’é¡¯ç¤º PAM (K-Medoids) åˆ†ç¾¤çµæœ
    3. æ¯å€‹ç¾¤çš„ä¸­å¿ƒä»¥ç´…è‰² X æ¨™ç¤º
    """
    
    # --- ğŸ§© æª¢æŸ¥è³‡æ–™æ ¼å¼ ---
    if data.ndim != 2 or data.shape[1] != 2:
        raise ValueError("âš ï¸ data å¿…é ˆæ˜¯ shape=(n_samples, 2) çš„ numpy array")
    
    # --- ğŸ§  è¨­å®šä¸­æ–‡å­—é«”ï¼ˆmacOS å¯ç”¨ Heiti TCï¼‰---
    plt.rcParams['font.family'] = 'Heiti TC'
    plt.rcParams['axes.unicode_minus'] = False

    # --- ğŸ¨ å»ºç«‹å­åœ– ---
    fig, axes = plt.subplots(2, len(k_values), figsize=(4 * len(k_values), 8), constrained_layout=True)
    axes = axes.ravel()

    # --- ğŸ”¹ K-Means ä¸Šæ’ ---
    for i, k in enumerate(k_values):
        kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)
        kmeans.fit(data)
        labels = kmeans.labels_
        centroids = kmeans.cluster_centers_

        axes[i].scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', s=40, edgecolors='k')
        axes[i].scatter(centroids[:, 0], centroids[:, 1], marker='X', s=200, c='red')
        axes[i].set_title(f'K-Means (k={k})', fontsize=12)
        axes[i].set_xlabel('Feature 1'); axes[i].set_ylabel('Feature 2'); axes[i].grid(True)

    # --- ğŸ”¹ PAM (K-Medoids) ä¸‹æ’ ---
    for i, k in enumerate(k_values):
        kmedoids = KMedoids(n_clusters=k, random_state=0, metric='euclidean')
        kmedoids.fit(data)
        labels = kmedoids.labels_
        medoids = kmedoids.cluster_centers_

        axes[i + len(k_values)].scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', s=40, edgecolors='k')
        axes[i + len(k_values)].scatter(medoids[:, 0], medoids[:, 1], marker='X', s=200, c='red')
        axes[i + len(k_values)].set_title(f'PAM (K-Medoids) (k={k})', fontsize=12)
        axes[i + len(k_values)].set_xlabel('Feature 1'); axes[i + len(k_values)].set_ylabel('Feature 2'); axes[i + len(k_values)].grid(True)

    # --- ğŸ§­ æ¨™è¨»æ•´é«”èªªæ˜ ---
    fig.suptitle('K-Means vs PAM (K-Medoids) åˆ†ç¾¤æ¯”è¼ƒ', fontsize=16, y=1.02)
    plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN

# -----------------------------
# ğŸ”¹ ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸ·è¡Œ DBSCAN ä¸¦å°å‡ºåˆ†ç¾¤çµæœ
# -----------------------------
def run_dbscan(data, eps=0.3, min_samples=10, scale=True):
    """
    åŸ·è¡Œ DBSCAN åˆ†ç¾¤ä¸¦è¼¸å‡ºåŸºæœ¬çµ±è¨ˆ
    åƒæ•¸:
        data : np.ndarray
            è¼¸å…¥è³‡æ–™ (n_samples, n_features)
        eps : float
            é„°è¿‘è·é›¢é–¾å€¼
        min_samples : int
            æœ€å°‘é„°å±…æ•¸
        scale : bool
            æ˜¯å¦é€²è¡Œæ¨™æº–åŒ– (StandardScaler)
    å›å‚³:
        db : è¨“ç·´å¥½çš„ DBSCAN æ¨¡å‹
        labels : åˆ†ç¾¤æ¨™ç±¤
    """
    # è³‡æ–™æ¨™æº–åŒ–ï¼ˆå¯é—œé–‰ï¼‰
    X = StandardScaler().fit_transform(data) if scale else data

    # åŸ·è¡Œ DBSCAN
    db = DBSCAN(eps=eps, min_samples=min_samples).fit(X)
    labels = db.labels_

    # çµ±è¨ˆè³‡è¨Š
    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise_ = list(labels).count(-1)

    print(f"Estimated number of clusters: {n_clusters_}")
    print(f"Estimated number of noise points: {n_noise_}")

    return db, labels, X


# -----------------------------
# ğŸ”¹ ç¬¬äºŒéƒ¨åˆ†ï¼šç•«å‡ºåˆ†ç¾¤çµæœåœ–
# -----------------------------
def plot_dbscan_clusters(db, labels, X):
    """
    ç•«å‡º DBSCAN çš„åˆ†ç¾¤çµæœ
    åƒæ•¸:
        db : å·²è¨“ç·´çš„ DBSCAN æ¨¡å‹
        labels : æ¨¡å‹åˆ†ç¾¤çµæœ
        X : (æ¨™æº–åŒ–å¾Œ) è³‡æ–™åº§æ¨™
    """
    unique_labels = set(labels)
    core_samples_mask = np.zeros_like(labels, dtype=bool)
    core_samples_mask[db.core_sample_indices_] = True

    plt.figure(figsize=(7, 6))
    colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]

    for k, col in zip(unique_labels, colors):
        if k == -1:
            # é»‘è‰²ä»£è¡¨é›œè¨Šé»
            col = [0, 0, 0, 1]

        class_member_mask = (labels == k)

        # æ ¸å¿ƒé»
        xy = X[class_member_mask & core_samples_mask]
        plt.plot(
            xy[:, 0],
            xy[:, 1],
            "o",
            markerfacecolor=tuple(col),
            markeredgecolor="k",
            markersize=14,
        )

        # éæ ¸å¿ƒé»
        xy = X[class_member_mask & ~core_samples_mask]
        plt.plot(
            xy[:, 0],
            xy[:, 1],
            "o",
            markerfacecolor=tuple(col),
            markeredgecolor="k",
            markersize=6,
        )

    plt.title(f"DBSCAN Clusters (n={len(set(labels)) - (1 if -1 in labels else 0)})")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.grid(True)
    plt.show()
